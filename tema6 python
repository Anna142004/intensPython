1)
import numpy as np

def f(x):
  return x[0] + 5 * x[1] + np.exp(x[2])

def grad(x):
  return np.array([1, 5, np.exp(x[2])])

def coordinate_descent(x0, eps, max_iter):
  x = x0.copy()
  for _ in range(max_iter):
    for i in range(len(x)):
      alpha = 0
      x_min = f(x)
      for alpha_candidate in np.arange(-1e-3, 1e-3, 1e-4):
        x_new = x.copy()
        x_new[i] += alpha_candidate
        f_new = f(x_new)
        if f_new < x_min:
          x_min = f_new
          alpha = alpha_candidate
      x[i] += alpha

    norm_grad = np.linalg.norm(grad(x))
    if norm_grad < eps:
      break

  return x

if name == "main":
  # Задать параметры оптимизации
  x0 = np.array([0, 0, 0])  # Начальное значение вектора параметров
  eps = 1e-6  # Допустимая погрешность
  max_iter = 1000  # Максимальное число итераций

  # Вычислить локальный минимум
  x_min = coordinate_descent(x0, eps, max_iter)

  # Вывести результат
  print("Локальный минимум:", x_min)
  print("Значение целевой функции:", f(x_min))

2)
import numpy as np

def f(x):
  """
  Функция f(x) = x1 + 5x2 + x3^2
  """
  x1, x2, x3 = x
  return x1 + 5 * x2 + x3**2

def gradient(x):
  """
  Градиент функции f(x) = x1 + 5x2 + x3^2
  """
  x1, x2, x3 = x
  return np.array([1, 5, 2 * x3])

def gradient_descent(x0, alpha, epsilon):
  """
  Метод наискорейшего спуска
  """
  x = x0
  while np.linalg.norm(gradient(x)) > epsilon:
    g = gradient(x)
    x -= alpha * g
  return x

# Задаем начальные параметры
x0 = np.array([0, 0, 0])
alpha = 0.1
epsilon = 1e-6

# Применяем метод наискорейшего спуска
x_min = gradient_descent(x0, alpha, epsilon)

# Выводим результат
print("Минимум функции f(x) = x1 + 5x2 + x3^2:")
print(x_min)
print("Значение функции в минимуме:", f(x_min))
